{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0761414e-735a-424e-b08d-c90f2d11a651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/azaelcarrillo/Documents/AutoML_in_data_augmentation/taks/PolitiES/model\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1d16bf3-6d12-4533-afd1-bc6337337433",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "targets = ['gender', 'profession','ideology_binary', 'ideology_multiclass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bda03104-8dfe-420f-bcc8-b59be68f02ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azaelcarrillo/miniconda3/envs/rest-mex-reco-gpu/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoTokenizer\n",
    "from pysentimiento.preprocessing import preprocess_tweet\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1s(preds, dtrain):\n",
    "    f1 = f1_score(dtrain.get_label(), np.round(preds), average='macro')\n",
    "    return 'f1', f1\n",
    "\n",
    "def get_train_test(data_train, reduce=False):\n",
    "    users_train, users_test = train_test_split(data_train.index.unique(), test_size=.2, random_state=19970808)\n",
    "    \n",
    "    if reduce:\n",
    "        data_train['idd'] = np.array(range(data_train.shape[0])) % reduce\n",
    "        df_tweets = data_train.reset_index().groupby(by=['label', 'idd']).tweet.apply(lambda x: ' '.join(x))\n",
    "        df = data_train.reset_index().groupby(by=['label', 'idd']).first()\n",
    "        \n",
    "        \n",
    "        data_test['idd'] = np.array(range(data_test.shape[0])) % reduce\n",
    "        df_test = data_test.reset_index().groupby(by=['label', 'idd']).tweet.apply(lambda x: ' '.join(x))\n",
    "        \n",
    "    else:\n",
    "        df = data_train\n",
    "        df_tweets = data_train.tweet\n",
    "    \n",
    "    df_targets = df[targets].apply(LabelEncoder().fit_transform)\n",
    "    \n",
    "    X_train = df_tweets.loc[users_train]\n",
    "    Y_train = df_targets.loc[users_train]\n",
    "    Y_train = Y_train.droplevel(1)\n",
    "\n",
    "    X_test = df_tweets.loc[users_test]\n",
    "    Y_test = df_targets.loc[users_test]\n",
    "    Y_test = Y_test.droplevel(1)\n",
    "    \n",
    "    return X_train, X_test, df_test, Y_train, Y_test\n",
    "\n",
    "# Bert utils\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=512)\n",
    "\n",
    "def fine_tune_beto(X_train, X_test, y_train, y_test, iters):\n",
    "    dataset_train = Dataset.from_pandas(pd.DataFrame({'text': X_train.values, 'labels': y_train.values})).shuffle()\n",
    "    dataset_test = Dataset.from_pandas(pd.DataFrame({'text': X_test.values, 'labels': y_test.values})).shuffle()\n",
    "\n",
    "    dataset_train_tok = dataset_train.map(preprocess_function, batched=True)\n",
    "    dataset_test_tok = dataset_test.map(preprocess_function, batched=True)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=iters,\n",
    "        weight_decay=0.01, \n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=bert_model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset_train_tok,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    old_collator = trainer.data_collator\n",
    "    trainer.data_collator = lambda data: dict(old_collator(data))\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "def get_embeddings(X):\n",
    "    X_emb = []\n",
    "    for txt in X:\n",
    "        encoding = tokenizer.encode_plus(txt, \n",
    "                                         add_special_tokens=True, \n",
    "                                         truncation=True, \n",
    "                                         padding = \"max_length\", \n",
    "                                         return_attention_mask=True, \n",
    "                                         return_tensors=\"pt\")\n",
    "\n",
    "        encoding = encoding.to(device)\n",
    "\n",
    "        output = bert_model.base_model(**encoding)\n",
    "        embeddings_tensor = output.pooler_output\n",
    "        embeddings = embeddings_tensor.tolist()[0]\n",
    "        X_emb.append(embeddings)\n",
    "    X_emb = np.array(X_emb)\n",
    "    \n",
    "    return X_emb\n",
    "\n",
    "def get_predict(X):\n",
    "    X_emb = []\n",
    "    for txt in X:\n",
    "        encoding = tokenizer.encode_plus(txt, \n",
    "                                         add_special_tokens=True, \n",
    "                                         truncation=True, \n",
    "                                         padding = \"max_length\", \n",
    "                                         return_attention_mask=True, \n",
    "                                         return_tensors=\"pt\")\n",
    "\n",
    "        encoding = encoding.to(device)\n",
    "\n",
    "        output = bert_model(**encoding)\n",
    "        pre = torch.argmax(output.logits).tolist()\n",
    "        \n",
    "        X_emb.append(pre)\n",
    "    X_emb = np.array(X_emb)\n",
    "    \n",
    "    return X_emb\n",
    "\n",
    "\n",
    "def create_submission(users, gender, prof, ideobi, ideomul):\n",
    "    header = ['user','gender','profession','ideology_binary','ideology_multiclass']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcb6d8f7-d48f-44ef-af39-3c61a79aa6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba5cb0c4-927f-464e-870b-0d5603dbbfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre = pd.read_csv(\"../data/raw/tweets_complete.csv\")\n",
    "data_pre.set_index('label', inplace=True)\n",
    "\n",
    "data_train = pd.read_csv(\"../data/raw/training.csv\")\n",
    "data_dev = pd.read_csv('../data/raw/development.csv')\n",
    "data_dev_test = pd.read_csv('../data/raw/development_test.csv')\n",
    "data_test = pd.read_csv(\"../data/raw/test_without_labels.csv\")\n",
    "\n",
    "data_test.drop(columns='Unnamed: 0', inplace=True)\n",
    "data_test.set_index('label', inplace=True)\n",
    "data_train.set_index('label', inplace=True)\n",
    "\n",
    "data_train['tweet'] = data_pre['tweet_emoji_signos_pre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f89620c5-4685-4bd3-a8c9-6f8ed2d4c36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, X_sub, Y_train, Y_test = get_train_test(data_train, reduce=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "965f995c-063e-4d90-9563-92e356b8bdfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaForSequenceClassification, AutoTokenizer, BertForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased')\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e512417-ddf5-4743-87dc-75f12997e3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_model = BertForSequenceClassification.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2a1118-b6b9-4ceb-9455-45ec136bbda8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  3.44ba/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.51ba/s]\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running training *****\n",
      "  Num examples = 3000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1125\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1125' max='1125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1125/1125 05:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.506300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.266500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-500\n",
      "Configuration saved in ./results/checkpoint-500/config.json\n",
      "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-1000\n",
      "Configuration saved in ./results/checkpoint-1000/config.json\n",
      "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 gender: 0.7945652173913043\n"
     ]
    }
   ],
   "source": [
    "tuned_embeddings = {}\n",
    "for target in targets[0:1]:\n",
    "    bert_model = BertForSequenceClassification.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased')\n",
    "    \n",
    "    y_train = Y_train[target]\n",
    "    y_test = Y_test[target]\n",
    "    fine_tune_beto(X_train, X_test, y_train, y_test, 3)\n",
    "\n",
    "    X_train_emb = get_embeddings(X_train)\n",
    "    X_test_emb = get_embeddings(X_test)\n",
    "    X_sub_emb = get_embeddings(X_sub)\n",
    "\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(X_train_emb, y_train)\n",
    "    y_pred = model.predict_proba(X_test_emb)[:,1]\n",
    "    y_pred = pd.Series(y_pred, y_test.index)\n",
    "\n",
    "    result = f1_score(y_test.groupby(level=0).mean(), y_pred.groupby(level=0).mean().round(), average='macro')\n",
    "    print('f1 {}: {}'.format(target, result))\n",
    "\n",
    "\n",
    "    y_pred_bert = get_predict(X_test)\n",
    "    y_pred_bert = pd.Series(y_pred_bert, y_test.index)\n",
    "    \n",
    "    y_sub_bert = get_predict(X_sub)\n",
    "    y_sub_bert = pd.Series(y_sub_bert, X_sub.index.get_level_values(0))\n",
    "    \n",
    "    \n",
    "    result = f1_score(y_test.groupby(level=0).mean(), y_pred_bert.groupby(level=0).mean().round(), average='macro')\n",
    "    print('f1 {}: {}'.format(target, result))\n",
    "    \n",
    "    \n",
    "    tuned_embeddings[target] = {}\n",
    "    tuned_embeddings[target]['embeddings'] = {'train': X_train_emb,\n",
    "                                              'test': X_test_emb,\n",
    "                                              'sub': X_sub_emb}\n",
    "    tuned_embeddings[target]['bert_pred'] = {'test': y_pred_bert, \n",
    "                                             'sub': y_sub_bert}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85024cd0-1927-41ca-9a42-c63c68ee7d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_emb = tuned_embeddings['gender']['embeddings']['train']\n",
    "X_test_emb = tuned_embeddings['gender']['embeddings']['test']\n",
    "X_sub_emb = tuned_embeddings['gender']['embeddings']['sub']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ffdafcb-9e77-4d77-b60d-7cb0eae4c7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1016a95-3c3f-4cda-bed0-95189cf3a38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "366274bd-6de3-47f1-bf65-5587e6da1963",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(analyzer = 'word',\n",
    "                     min_df = 1,\n",
    "                     max_features = 5000,\n",
    "                     lowercase=True,\n",
    "                     stop_words=stopwords)\n",
    "X_train_tf = tf.fit_transform(X_train).toarray()\n",
    "X_test_tf = tf.transform(X_test).toarray()\n",
    "X_sub_tf = tf.transform(X_sub).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32c5a9f6-e0c7-42f8-a55e-32cdd0af10db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full = np.concatenate((X_train_emb, X_train_tf), axis=1)\n",
    "X_test_full = np.concatenate((X_test_emb, X_test_tf), axis=1)\n",
    "X_sub_full = np.concatenate((X_sub_emb, X_sub_tf), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58707dbe-fa79-414c-b1f2-3da2e83ac0d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7904656319290465"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = Y_train['gender']\n",
    "y_test = Y_test['gender']\n",
    "\n",
    "param_grid_reg = [\n",
    "    {\"C\": [.0001, .001, .001, 0.1, 1, 2, 5, 10],\n",
    "     \"penalty\": [\"l1\", \"l2\"]\n",
    "     }\n",
    "    ]\n",
    "\n",
    "model = LogisticRegression(max_iter=2000, solver='liblinear', C=.001, penalty='l1')\n",
    "model.fit(X_train_full, y_train)\n",
    "\n",
    "gender_pred = model.predict_proba(X_test_full)[:,1]\n",
    "gender_pred = pd.Series(gender_pred, X_test.index)\n",
    "\n",
    "f1_score(y_test.groupby(level=0).mean(), gender_pred.groupby(level=0).mean().round(), average='macro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
